> 스터디 시간동안 공유된 내용을 정리합니다. 더 추가하고 싶은 답변, 내용은 자유롭게 더해주세요!

## 0. 강의 들어본 소감

- 인공지능과 해당 강의의 개념이 어떤 연관성을 갖는지에 대한 부분이 부족하다.   
➡️ 스터디 시간에 흐릿하게 연결되는 **인공지능과의 연결고리**를 찾은 후, 조사해오기로 했습니다.

## 1. 질문

#### Q. 베이즈 정리에서 나오는 observation, 이미 관측된 데이터가 뜻하는 의미가 무엇인가요?

A. '사전에 알게된 확률' 을 뜻한다. 어떤 경험을 통해 얻어진 값이 있고, 특정 사건과 이미 관측된 데이터의 관계성*(이미 관측된 데이터가 특정 사건으로 부터 온 가능성)*을 계산할 때, 베이즈 정리를 이용해 조건부 확률을 계산할 수 있다. 
* 강의의 통신 채널을 예로 들면, 0 혹은 1 이라는 신호를 받았을 때, input으로 어떤 신호가 들어왔는지(output의 원인)를 계산하는 것이다. 정상 작동했을 경우와 에러가 났을 경우는 0, 1 신호를 통해 실험으로 얻어낼 수 있기 때문에 observation에 해당된다. 이 데이터를 통해 unconditioned error의 확률을 계산할 수 있다.  


#### Q. 이항정리에서의 계수가 무엇을 뜻하나요?

A. $(a+b)^n$ 이 있다면, 한 주머니에 $a$ 라는 공과 $b$  라는 공이 있다는 상황이라고 할 수 있다. 계수는 복원 추출을 하기 위한 값이다. *(복원추출 : 주머니 안의 총 공의 갯수 n을 유지하면서 공을 꺼내는 것)* 
* 해당 질문은 진표님이 공유해주신 **아래 블로그의 예제**를 같이 풀이하면서 해결할 수 있었습니다. 

[확률과 통계 06탄] 이항정리 계수](https://j1w2k3.tistory.com/747)


#### Q. 이항정리의 미분

A. 아이펠SSAC의  @인학 장 님께서 답변해주셨습니다.

f(x)=(1+x)^n, f'=n(1+x)^{n-1}  미분을 한 후, 양 변에 x=1 을 대입하면 이항분포에서 평균과 분산을 구할 때 이용할 수 있습니다.
이항분포에서 확률 변수는 $X=r$ 이고, $(q+pt)^n$ 을 미분하면 확률변수와 확률의 곱으로 나타낼 수 있습니다. 이를 통해 평균을 구할 수 있습니다.
이차미분(이계도함수)를 하면 분산을 구할 수 있습니다.

- 추가로 알아두면 좋은 이항정리 계수    
2_nC_0 + 2_nC_2 + ... + 2_nC_n
= 2_nC_1 + 2_nC_3 + ... + 2_nC_{n-1}
= 2^{2n-1}


## 2. (공부하면) 해결될 질문 (== 👩🏻‍💻 조사해 올 것들)



#### Q. 베이즈 정리, 조건부확률 confusion matrix와 관계 있나요?
- 오차행렬: 분류 모델이 정확한지 평가할때 사용하는 행렬      
- 우리가 만든 분류 모델의 예측이 실제와 맞았는지 평가(정확율)하고 기계의 예측이 실제값이 될 확률(재현율)     
- 조건부 확률: 어떤 사건 a가 일어났을때, 사건 b가 일어날 확률       
  - [참고링크](https://horizon.kias.re.kr/7536/)     
- 조건부 확률을 사용해서 오차행렬을 구한다고 할 수 있음      
- 조건부 확률이 인공지능에 쓰이는 경우 = 조건부 확률에서 구하는 확률을 높이기 위해 많은 통계 데이터가 필요하다.      
즉, 인공지능에서 기계의 정확도를 높이기 위해 과거 데이터가 많이 필요하다. 이를 통해 앞으로 일어날 일을 예측하거나 그 원인을 더 정확하게 찾을 수 있다.     
  
    ---


#### Q. markov chain과 트랜스포머, 관계가 있나요?**
- 딥러닝 전에 맥락을 어떻게 알았을까? : [쉽게 설명된 유투브 링크](https://www.youtube.com/watch?v=-VhuXeWFEBU)      
- GAN에서도 언급되는 개념     
![image](https://user-images.githubusercontent.com/68461606/112925872-8b450e00-914d-11eb-80b3-aac24a41fe72.png)  

    ---


#### Q. 이항정리 미분을 왜 하나요?**
- 이항정리의 활용   
  - 직접 곱하기 전개계산을 안 해도, 계수를 알아낼 수 있다. ⇒ 조합 공식이니까!   
  - 미분공식을 증명할 때 활용 가능하다   
      - 멱공식= x^n의 미분 ⇒ d/dx(x^n) = n* x^(n-1), n: 양의 정수   
  - 이외에도, 무리함수, 로그함수, pi 같은 무한으로 가는? 식이나 수를 구하는데 활용 가능하다.   
    ---


#### Q. stirling's formula 와 인공지능, 관계가 있을 것만 같아요!**   
- 근사 함수 : 모델의 강건성을 위해서 사용한다.   
- 노이즈나 다량의 데이터에 대응하기 위함   
- 데이터의 정확한 정보가 아닌 전체 데이터의 경향성을 파악   
- 인공신경망 : 실제 문제를 해결하기 위한 근사 함수를 구하는 방법   
출처 : [인공 신경망](https://ko.wikipedia.org/wiki/인공_신경망)   

    ---


#### Q. 조건부 확률과 자연어처리, 관계가 있나요?**
- 언어 모델의 전통적인 방법인 통계 모델(Statistical Language Model, SLM) 은 조건부 확률과 밀접한 관련이 있습니다. : [출처](https://wikidocs.net/21687)   
**N-gram**   
- 기존 데이터에서 등장하는 단어의 빈도수를 세는 방식의 언어 모델 : Language Model   
ex) bag-of-word [https://wikidocs.net/22650](https://wikidocs.net/22650)   
     → 세상엔 무수히 많은 단어 조합이 있기 때문에 새로운 문장이 발견되면 확률이 0으로 계산된다.   
     → 학습 데이터의 수가 많아질 수록 전체 경우의 수를 고려할 수 없어짐    
- 이런 문제를 해결하기 위해 n개의 연속된 단어를 묶어 생각하는 N-gram 모델이 연구되었다.   
[reference](https://jiho-ml.com/weekly-nlp-14/)   


